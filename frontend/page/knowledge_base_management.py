import streamlit as st
import os
import sys
import pandas as pd
from typing import Dict, List, Any, Optional
import uuid
import tempfile
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
sys.path.append(project_root)

from frontend.ui_components import show_sidebar, show_footer, apply_common_styles
from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import tool
from utils.llm_tools import init_language_model, CustomEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langfuse.callback import CallbackHandler

# st.query_params.role = st.session_state.role

# åº”ç”¨è‡ªå®šä¹‰æ ·å¼
apply_common_styles()

# æ˜¾ç¤ºä¾§è¾¹æ 
show_sidebar()

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if "csv_file" not in st.session_state:
    st.session_state.csv_file = None
if "loaded_documents" not in st.session_state:
    st.session_state.loaded_documents = []
if "session_id" not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())
if "vector_store" not in st.session_state:
    st.session_state.vector_store = None
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "current_tab" not in st.session_state:
    st.session_state.current_tab = "çŸ¥è¯†åº“æ„å»º"
if "top_k" not in st.session_state:
    st.session_state.top_k = 5


def create_langfuse_handler(session_id: str, step: str) -> CallbackHandler:
    """
    åˆ›å»ºLangfuseå›è°ƒå¤„ç†å™¨

    Args:
        session_id: ä¼šè¯ID
        step: å¤„ç†æ­¥éª¤

    Returns:
        Langfuseå›è°ƒå¤„ç†å™¨
    """
    return CallbackHandler(
        tags=["knowledge_base_qa"],
        session_id=session_id,
        metadata={"step": step},
    )


def main():
    """ä¸»å‡½æ•°ï¼ŒåŒ…å«åº”ç”¨çš„ä¸»è¦é€»è¾‘å’ŒUIç»“æ„"""
    st.title("ğŸ“š çŸ¥è¯†åº“å†…å®¹ç®¡ç†ä¸é—®ç­”")
    st.markdown("---")

    # ä½¿ç”¨æ ‡ç­¾é¡µç»„ç»‡ä¸åŒåŠŸèƒ½ï¼Œåˆå¹¶å‰ä¸¤ä¸ªæ ‡ç­¾é¡µ
    tabs = st.tabs(["çŸ¥è¯†åº“æ„å»º", "æ£€ç´¢æŸ¥çœ‹", "é—®ç­”åŠ©æ‰‹"])

    # çŸ¥è¯†åº“æ„å»ºæ ‡ç­¾é¡µï¼ˆåˆå¹¶åŸæ¥çš„ä¸Šä¼ æ–‡æ¡£å’Œæ„å»ºçŸ¥è¯†åº“ï¼‰
    with tabs[0]:
        if st.session_state.current_tab != "çŸ¥è¯†åº“æ„å»º":
            st.session_state.current_tab = "çŸ¥è¯†åº“æ„å»º"
        display_info_message()
        display_workflow()
        
        # ä¸Šä¼ æ–‡æ¡£éƒ¨åˆ†
        st.markdown("## æ•°æ®ä¸Šä¼ ")
        handle_file_upload()
        if st.session_state.csv_file:
            process_csv_file()
            preview_loaded_documents()
        
        # æ„å»ºçŸ¥è¯†åº“éƒ¨åˆ†
        st.markdown("## çŸ¥è¯†åº“æ„å»º")
        if not st.session_state.loaded_documents:
            st.warning("è¯·å…ˆä¸Šä¼ å¹¶å¤„ç†CSVæ–‡ä»¶")
        else:
            build_knowledge_base_ui()

    # æ£€ç´¢æŸ¥çœ‹æ ‡ç­¾é¡µ
    with tabs[1]:
        if st.session_state.current_tab != "æ£€ç´¢æŸ¥çœ‹":
            st.session_state.current_tab = "æ£€ç´¢æŸ¥çœ‹"
        retrieval_interface()

    # é—®ç­”åŠ©æ‰‹æ ‡ç­¾é¡µ
    with tabs[2]:
        if st.session_state.current_tab != "é—®ç­”åŠ©æ‰‹":
            st.session_state.current_tab = "é—®ç­”åŠ©æ‰‹"
        qa_interface()

    show_footer()


def display_info_message():
    """æ˜¾ç¤ºçŸ¥è¯†åº“å†…å®¹ç®¡ç†çš„ä¿¡æ¯æ¶ˆæ¯"""
    st.info(
        """
        æœ¬å·¥å…·æ”¯æŒä»CSVæ–‡ä»¶ä¸­åŠ è½½çŸ¥è¯†åº“å†…å®¹ã€‚æ‚¨å¯ä»¥ä¸Šä¼ CSVæ–‡ä»¶ï¼Œç³»ç»Ÿå°†ä½¿ç”¨LangChainçš„æ–‡æ¡£åŠ è½½å™¨å¤„ç†æ•°æ®ï¼Œ
        å¹¶æä¾›é¢„è§ˆåŠŸèƒ½ã€‚ä¸Šä¼ çš„å†…å®¹å°†è¢«ç”¨äºæ„å»ºçŸ¥è¯†åº“ï¼Œæ”¯æŒåç»­çš„æ£€ç´¢å’Œé—®ç­”åŠŸèƒ½ã€‚
        """
    )


def display_workflow():
    """æ˜¾ç¤ºçŸ¥è¯†åº“å†…å®¹ç®¡ç†çš„å·¥ä½œæµç¨‹"""
    with st.expander("ğŸ“‹ æŸ¥çœ‹çŸ¥è¯†åº“å†…å®¹ç®¡ç†å·¥ä½œæµç¨‹", expanded=False):
        st.markdown(
            """
            1. **æ•°æ®ä¸Šä¼ **
               - æ”¯æŒCSVæ–‡ä»¶ä¸Šä¼ 
               - è‡ªåŠ¨æå–æ–‡æ¡£å†…å®¹
            
            2. **çŸ¥è¯†åº“æ„å»º**
               - å°†æ–‡æ¡£åˆ†å‰²æˆé€‚å½“å¤§å°
               - æ–‡æ¡£å‘é‡åŒ–å¹¶å­˜å…¥å‘é‡æ•°æ®åº“
            
            3. **æ™ºèƒ½é—®ç­”**
               - åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æŠ€æœ¯
               - æ”¯æŒä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¯¹è¯å¼äº¤äº’
            """
        )


def handle_file_upload():
    """å¤„ç†æ–‡ä»¶ä¸Šä¼ é€»è¾‘"""
    with st.container(border=True):
        uploaded_file = st.file_uploader(
            "ä¸Šä¼ CSVæ–‡ä»¶",
            type=["csv"],
            accept_multiple_files=False,
        )
        
        if uploaded_file:
            st.session_state.csv_file = uploaded_file
            st.success(f"æˆåŠŸä¸Šä¼ æ–‡ä»¶: {uploaded_file.name}")


def process_csv_file():
    """å¤„ç†ä¸Šä¼ çš„CSVæ–‡ä»¶ï¼Œä½¿ç”¨LangChainçš„æ–‡æ¡£åŠ è½½å™¨"""
    if st.session_state.csv_file:
        with st.spinner("æ­£åœ¨å¤„ç†CSVæ–‡ä»¶..."):
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(delete=False, suffix='.csv') as tmp_file:
                tmp_file.write(st.session_state.csv_file.getvalue())
                tmp_path = tmp_file.name
            
            try:
                # ç›´æ¥åŠ è½½æ–‡æ¡£ï¼Œä¸ä½¿ç”¨é¢å¤–çš„æŒ‰é’®
                loader = CSVLoader(file_path=tmp_path)
                documents = loader.load()
                
                # å­˜å‚¨å¤„ç†ç»“æœ
                st.session_state.loaded_documents = documents
                st.success(f"æˆåŠŸåŠ è½½ {len(documents)} æ¡æ–‡æ¡£è®°å½•")
            
            except Exception as e:
                st.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            
            finally:
                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(tmp_path):
                    os.unlink(tmp_path)


def preview_loaded_documents():
    """é¢„è§ˆåŠ è½½çš„æ–‡æ¡£å†…å®¹"""
    if st.session_state.loaded_documents:
        st.markdown("## æ–‡æ¡£é¢„è§ˆ")
        with st.container(border=True):
            # æ˜¾ç¤ºæ–‡æ¡£æ•°é‡
            st.write(f"å…±åŠ è½½äº† {len(st.session_state.loaded_documents)} æ¡æ–‡æ¡£è®°å½•")
            
            # åˆ›å»ºæ–‡æ¡£é¢„è§ˆ
            preview_data = []
            for i, doc in enumerate(st.session_state.loaded_documents[:10]):  # åªé¢„è§ˆå‰10æ¡
                preview_data.append({
                    "åºå·": i + 1,
                    "å†…å®¹é¢„è§ˆ": doc.page_content[:100] + "..." if len(doc.page_content) > 100 else doc.page_content,
                    "å…ƒæ•°æ®": str(doc.metadata)
                })
            
            # æ˜¾ç¤ºé¢„è§ˆè¡¨æ ¼
            st.dataframe(
                pd.DataFrame(preview_data),
                use_container_width=True
            )
            
            if len(st.session_state.loaded_documents) > 10:
                st.info(f"ä»…æ˜¾ç¤ºå‰10æ¡è®°å½•ï¼Œå…± {len(st.session_state.loaded_documents)} æ¡")
            
            # æä¾›è¯¦ç»†æŸ¥çœ‹é€‰é¡¹
            with st.expander("æŸ¥çœ‹å®Œæ•´æ–‡æ¡£å†…å®¹", expanded=False):
                doc_index = st.number_input(
                    "é€‰æ‹©è¦æŸ¥çœ‹çš„æ–‡æ¡£åºå·", 
                    min_value=1, 
                    max_value=len(st.session_state.loaded_documents),
                    value=1
                )
                
                if doc_index:
                    selected_doc = st.session_state.loaded_documents[doc_index - 1]
                    st.write("### æ–‡æ¡£å†…å®¹")
                    st.write(selected_doc.page_content)
                    st.write("### å…ƒæ•°æ®")
                    st.json(selected_doc.metadata)


def build_knowledge_base_ui():
    """æ„å»ºå‘é‡æ•°æ®åº“UIç»„ä»¶"""
    with st.container(border=True):
        st.info("é€šè¿‡è¿™ä¸ªæ­¥éª¤ï¼Œç³»ç»Ÿå°†å¤„ç†å·²åŠ è½½çš„æ–‡æ¡£ï¼Œåˆ†å‰²æˆé€‚å½“å¤§å°ï¼Œå¹¶æ„å»ºå‘é‡å­˜å‚¨ä»¥æ”¯æŒè¯­ä¹‰æ£€ç´¢")
        
        col1, col2 = st.columns(2)
        with col1:
            chunk_size = st.number_input("æ–‡æ¡£åˆ†å‰²å¤§å°", min_value=100, max_value=2000, value=500, step=100)
        with col2:
            chunk_overlap = st.number_input("åˆ†å‰²é‡å å¤§å°", min_value=0, max_value=500, value=50, step=10)
        
        embedding_model_name = os.getenv("EMBEDDING_MODEL", "BAAI/bge-m3")
        
        if st.button("å¼€å§‹æ„å»ºçŸ¥è¯†åº“"):
            with st.spinner("æ­£åœ¨æ„å»ºå‘é‡æ•°æ®åº“..."):
                try:
                    # 1. æ–‡æ¡£åˆ†å‰²
                    text_splitter = RecursiveCharacterTextSplitter(
                        chunk_size=chunk_size, 
                        chunk_overlap=chunk_overlap
                    )
                    splits = text_splitter.split_documents(st.session_state.loaded_documents)
                    
                    # 2. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
                    embeddings = CustomEmbeddings(
                        api_key=os.getenv("EMBEDDING_API_KEY", ""),
                        api_url=os.getenv("EMBEDDING_API_BASE", ""),
                        model=embedding_model_name,
                    )
                    
                    # 3. åˆ›å»ºå‘é‡å­˜å‚¨
                    from langchain_community.vectorstores import FAISS
                    vector_store = FAISS.from_documents(splits, embeddings)
                    st.session_state.vector_store = vector_store
                    
                    st.success(f"æˆåŠŸæ„å»ºçŸ¥è¯†åº“ï¼å…±ç´¢å¼•äº† {len(splits)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
                except Exception as e:
                    st.error(f"æ„å»ºçŸ¥è¯†åº“æ—¶å‡ºé”™: {str(e)}")


def qa_interface():
    """é—®ç­”ç•Œé¢"""
    st.markdown("## çŸ¥è¯†åº“é—®ç­”åŠ©æ‰‹")
    
    if st.session_state.vector_store is None:
        st.warning("è¯·å…ˆæ„å»ºçŸ¥è¯†åº“")
        return
    
    # æ·»åŠ æ¸…ç©ºèŠå¤©å†å²çš„æŒ‰é’®
    if st.button("æ¸…ç©ºèŠå¤©å†å²", key="clear_chat"):
        st.session_state.chat_history = []
        st.rerun()
    
    # åˆ›å»ºä¸€ä¸ªchatå®¹å™¨å’Œåº•éƒ¨è¾“å…¥æ¡†å®¹å™¨
    chat_container = st.container()
    # æ·»åŠ ä¸€ä¸ªéšå½¢çš„å ä½ç¬¦ï¼Œç”¨äºå¡«å……ç©ºé—´
    spacer = st.empty()
    input_container = st.container()
    
    # åœ¨åº•éƒ¨è¾“å…¥å®¹å™¨ä¸­æ˜¾ç¤ºè¾“å…¥æ¡†
    with input_container:
        prompt = st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜:")
        
    # åœ¨èŠå¤©å®¹å™¨ä¸­æ˜¾ç¤ºèŠå¤©å†å²
    with chat_container:
        # æ˜¾ç¤ºå†å²æ¶ˆæ¯
        if not st.session_state.chat_history:
            st.info("åœ¨ä¸‹æ–¹è¾“å…¥é—®é¢˜ï¼Œå¼€å§‹ä¸çŸ¥è¯†åº“å¯¹è¯")
        
        for message in st.session_state.chat_history:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
        
        # å¤„ç†ç”¨æˆ·è¾“å…¥
        if prompt:
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²è®°å½•
            st.session_state.chat_history.append({"role": "user", "content": prompt})
            
            # ç«‹å³æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
            with st.chat_message("user"):
                st.markdown(prompt)
            
            # å¤„ç†ç”¨æˆ·é—®é¢˜å¹¶æ˜¾ç¤ºåŠ©æ‰‹å›å¤
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                message_placeholder.markdown("æ€è€ƒä¸­...")
                
                try:
                    # åŸºäºRAGçš„é—®ç­”
                    answer = process_rag_query(prompt)
                    
                    # æ›´æ–°æ¶ˆæ¯å ä½ç¬¦
                    message_placeholder.markdown(answer)
                    
                    # æ·»åŠ å›å¤åˆ°å†å²è®°å½•
                    st.session_state.chat_history.append({"role": "assistant", "content": answer})
                    
                except Exception as e:
                    message_placeholder.error(f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {str(e)}")
            
            # å¼ºåˆ¶é‡æ–°æ¸²æŸ“èŠå¤©å†å²
            st.rerun()


def display_chat_history():
    """æ˜¾ç¤ºèŠå¤©å†å²"""
    if not st.session_state.chat_history:
        st.info("åœ¨ä¸‹æ–¹è¾“å…¥é—®é¢˜ï¼Œå¼€å§‹ä¸çŸ¥è¯†åº“å¯¹è¯")
        return


def process_rag_query(query: str) -> str:
    """å¤„ç†åŸºäºRAGçš„é—®ç­”æŸ¥è¯¢"""
    # 1. è·å–å†å²å¯¹è¯
    chat_history = get_chat_history_context()
    
    # 2. åˆå§‹åŒ–è¯­è¨€æ¨¡å‹
    llm = init_language_model(temperature=0.0)
    
    # 3. æ‰§è¡Œæ£€ç´¢
    retrieved_docs = st.session_state.vector_store.similarity_search(query, k=st.session_state.top_k)
    context = "\n\n".join([
        f"æ¥æº: {doc.metadata}\nå†…å®¹: {doc.page_content}"
        for doc in retrieved_docs
    ])
    
    # åˆ›å»ºLangfuseå›è°ƒå¤„ç†å™¨
    langfuse_handler = create_langfuse_handler(
        st.session_state.session_id, 
        "knowledge_base_rag_query"
    )
    
    # 4. æ„å»ºæç¤º
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", """
        ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½é—®ç­”åŠ©æ‰‹ã€‚è¯·ä½¿ç”¨ä»¥ä¸‹æ£€ç´¢åˆ°çš„å†…å®¹æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
        å¦‚æœæ— æ³•ä»æ£€ç´¢å†…å®¹ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œè¯·ç›´æ¥è¯´æ˜ä½ ä¸çŸ¥é“ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯ã€‚
        å›ç­”åº”è¯¥ç®€æ´æ˜äº†ï¼Œæœ€å¤šä¸‰å¥è¯ã€‚
        
        æ£€ç´¢å†…å®¹:
        {context}
        
        å†å²å¯¹è¯:
        {chat_history}
        """),
        ("human", "{query}")
    ])
    
    # 5. ç”Ÿæˆå›ç­”ï¼Œæ·»åŠ Langfuseç›‘æ§
    chain = prompt_template | llm
    response = chain.invoke(
        {
            "context": context,
            "chat_history": chat_history,
            "query": query
        },
        config={"callbacks": [langfuse_handler]},
    )
    
    return response.content


def get_chat_history_context() -> str:
    """è·å–å¯¹è¯å†å²ä½œä¸ºä¸Šä¸‹æ–‡"""
    if len(st.session_state.chat_history) <= 1:
        return "æ— å†å²å¯¹è¯"
    
    # åªå–æœ€è¿‘5è½®å¯¹è¯
    recent_history = st.session_state.chat_history[-10:]
    formatted_history = []
    
    for message in recent_history:
        role = "ç”¨æˆ·" if message["role"] == "user" else "åŠ©æ‰‹"
        formatted_history.append(f"{role}: {message['content']}")
    
    return "\n".join(formatted_history)


def retrieval_interface():
    """æ£€ç´¢æŸ¥çœ‹ç•Œé¢ï¼Œç›´æ¥æ˜¾ç¤ºä»å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢çš„æ–‡æ¡£ç‰‡æ®µ"""
    st.markdown("## çŸ¥è¯†åº“ç›´æ¥æ£€ç´¢")
    
    if st.session_state.vector_store is None:
        st.warning("è¯·å…ˆæ„å»ºçŸ¥è¯†åº“")
        return
    
    with st.container(border=True):
        st.info("æœ¬åŠŸèƒ½å…è®¸æ‚¨è¾“å…¥æŸ¥è¯¢ï¼Œç›´æ¥æŸ¥çœ‹ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢åˆ°çš„æœ€ç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼Œä¸ç»è¿‡å¤§æ¨¡å‹å¤„ç†")
        
        # æŸ¥è¯¢è¾“å…¥å’Œå‚æ•°è®¾ç½® - æ”¹ä¸ºå‚ç›´æ’åˆ—
        query = st.text_input("è¯·è¾“å…¥æŸ¥è¯¢å†…å®¹:")
        search_button = st.button("æ£€ç´¢", key="retrieval_search")
        
        # æ›´æ–°ä¸ºé»˜è®¤å€¼5ï¼Œå¹¶ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€ä¸­
        st.session_state.top_k = st.slider("æ˜¾ç¤ºå‰Næ¡ç»“æœ", min_value=1, max_value=10, value=5)
        
        # æ£€ç´¢é€»è¾‘ï¼šå½“æŒ‰ä¸‹Enteré”®æˆ–ç‚¹å‡»æ£€ç´¢æŒ‰é’®æ—¶è§¦å‘
        if query and (search_button or st.session_state.get("_last_query") != query):
            # ä¿å­˜å½“å‰æŸ¥è¯¢ä»¥é¿å…é‡å¤æ‰§è¡Œ
            st.session_state._last_query = query
            
            with st.spinner("æ­£åœ¨æ£€ç´¢ç›¸å…³å†…å®¹..."):
                try:
                    # æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢
                    retrieved_docs = st.session_state.vector_store.similarity_search_with_score(query, k=st.session_state.top_k)
                    
                    if retrieved_docs:
                        st.success(f"æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(retrieved_docs)} æ¡ç›¸å…³å†…å®¹")
                        
                        # æ˜¾ç¤ºæ£€ç´¢ç»“æœ - æ‰€æœ‰ç»“æœé»˜è®¤å±•å¼€
                        for i, (doc, score) in enumerate(retrieved_docs):
                            with st.expander(f"ç»“æœ #{i+1} (ç›¸ä¼¼åº¦: {1-score:.4f})", expanded=True):
                                st.markdown(f"### ã€æ–‡æ¡£ {i+1}ã€‘")
                                st.markdown(doc.page_content)
                    else:
                        st.info("æœªæ‰¾åˆ°ç›¸å…³å†…å®¹")
                        
                except Exception as e:
                    st.error(f"æ£€ç´¢è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")


# æ·»åŠ å…¼å®¹æ€§å‡½æ•°
def build_knowledge_base():
    """æ„å»ºå‘é‡æ•°æ®åº“ï¼ˆä¿ç•™ä¸ºå…¼å®¹æ€§ï¼‰"""
    build_knowledge_base_ui()


if __name__ == "__main__":
    main()